{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empathy Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import *\n",
    "from statistics import *\n",
    "from math import *\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn import svm, tree, linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pd.options.mode.chained_assignment = None  \n",
    "\n",
    "data = pd.read_csv(\"responses.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding strings\n",
    "encoding = {\"Smoking\": {\"never smoked\": 1, \"tried smoking\": 2, \"former smoker\": 3, \"current smoker\": 4},\n",
    "            \"Alcohol\": {\"never\": 1, \"social drinker\": 2, \"drink a lot\": 3},\n",
    "            \"Punctuality\": {\"i am often early\": 1, \"i am often early\": 2, \"i am often running late\": 3, \"i am always on time\": 4},\n",
    "            \"Lying\": {\"never\": 1, \"sometimes\": 2, \"only to avoid hurting someone\": 3, \"everytime it suits me\": 4},\n",
    "            \"Internet usage\": {\"no time at all\": 1, \"less than an hour a day\": 2, \"few hours a day\": 3, \"most of the day\": 4},\n",
    "            \"Gender\": {\"male\": 1, \"female\": 2},\n",
    "            \"Left - right handed\": {\"right handed\": 1, \"left handed\": 2},\n",
    "            \"Education\": {\"primary school\": 1, \"currently a primary school pupil\": 1, \"secondary school\": 2, \"college/bachelor degree\": 3, \"masters degree\": 4, \"doctorate degree\": 5},\n",
    "            \"Only child\": {\"no\": 1, \"yes\": 2},\n",
    "            \"Village - town\": {\"village\": 1, \"city\": 2},\n",
    "            \"House - block of flats\": {\"block of flats\": 1, \"house/bungalow\": 2}}\n",
    "data.replace(encoding, inplace=True)\n",
    "\n",
    "# Removing rows with class label undefined\n",
    "data = data[pd.notnull(data['Empathy'])]\n",
    "\n",
    "# Filling the missing values with rounded mean for the column\n",
    "data.fillna(round(data.mean()), inplace = True)\n",
    "\n",
    "# Encoding class to zeros and ones\n",
    "data['Class'] = 42\n",
    "data['Class'].loc[data['Empathy'] == 1] = 0\n",
    "data['Class'].loc[data['Empathy'] == 2] = 0\n",
    "data['Class'].loc[data['Empathy'] == 3] = 0\n",
    "data['Class'].loc[data['Empathy'] == 4] = 1\n",
    "data['Class'].loc[data['Empathy'] == 5] = 1\n",
    "data.drop(['Empathy'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalizing data using 2 ways and see which normalizing is best**\n",
    "\n",
    "1- Normalized Between zero and one by max of each column\n",
    "\n",
    "2- Catagorical from 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into x and y\n",
    "x = data.values[:,0:149]\n",
    "y = data.values[:,149]\n",
    "\n",
    "# Normalizing the data between 0 and 1 by MaxScaler\n",
    "scaler = MaxAbsScaler() \n",
    "xNormalized = scaler.fit_transform(x)\n",
    "\n",
    "# Normalizing the data between 1 and 5, rounded to nearest interger.\n",
    "relacs = MinMaxScaler(feature_range=(1, 5)) \n",
    "data['Number of siblings'] = relacs.fit_transform(((data['Number of siblings'] - 0)/(10)).values.reshape(-1, 1)).round()\n",
    "data['Height'] = relacs.fit_transform(((data['Height'] - 60)/(205-60)).values.reshape(-1, 1)).round()\n",
    "data['Weight'] = relacs.fit_transform(((data['Weight'] - 40)/(165-40)).values.reshape(-1, 1)).round()\n",
    "data['Age'] = relacs.fit_transform(((data['Age'] - 15)/(30-15)).values.reshape(-1, 1)).round()\n",
    "\n",
    "xCatagorical = data.values[:,0:149]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** Spliting data into train, test and validate **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "xOrignalTrain, xOrignalTest, xNormalizedTrain, xNormalizedTest, xCatagoricalTrain, xCatagoricalTest,yTrain, yTest = train_test_split(\n",
    "    x, xNormalized, xCatagorical, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifer Selection and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am testing alot of classifers to see which classifer does best on which data. I have a rough idea what classifer is best for which data type but let see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results will be generated using 10 fold CV\n",
      "----------------------------------------------\n",
      "Linear Seperator\n",
      "-----------------\n",
      "Perceptron - Orignal Data Accuracy =  57.3313017659\n",
      "Perceptron - Normalized Data Accuracy =  69.0315088295\n",
      "Perceptron - Catagorical Data Accuracy =  68.397171433\n",
      "SVM Linear - Orignal Data Accuracy =  63.9305360213\n",
      "SVM Linear - Normalized Data Accuracy =  70.6644202219\n",
      "SVM Linear - Catagorical Data Accuracy =  66.033091108\n",
      "Non-Linear Seperator\n",
      "--------------------\n",
      "Decision Tree - Orignal Data Accuracy =  63.5670807939\n",
      "Decision Tree - Normalized Data Accuracy =  64.3264963276\n",
      "Decision Tree - Catagorigal Data Accuracy =  65.6891897171\n",
      "KNN - Orignal Data Accuracy =  64.5668854509\n",
      "KNN - Normalized Data Accuracy =  65.4279574934\n",
      "KNN - Catagorical Data Accuracy =  64.9342084701\n",
      "SVM RBF - Orignal Data Accuracy =  70.647171433\n",
      "SVM RBF - Normalized Data Accuracy =  67.7860603219\n",
      "SVM RBF - Catagorical Data Accuracy =  73.5131075168\n",
      "Probabistic Model\n",
      "-----------------\n",
      "NB - Original Data Accuracy =  65.9343256759\n",
      "NB - Catagorical Data Accuracy =  68.2908852946\n",
      "Ensemble Method\n",
      "---------------\n",
      "Random Forests- Orignal Data Accuracy =  70.5300046882\n",
      "Random Forests- Normalized Data Accuracy =  70.5300046882\n",
      "Random Forests- Catagorical Data Accuracy =  69.5315478981\n"
     ]
    }
   ],
   "source": [
    "# Testing Different Models on Different Datasets.\n",
    "print(\"All results will be generated using 10 fold CV\")\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "\n",
    "print(\"Linear Seperator\")\n",
    "print('-----------------')\n",
    "\n",
    "# Perceptron\n",
    "PERC = Perceptron(tol=1e-3, random_state=0)\n",
    "print(\"Perceptron - Orignal Data\", \"Accuracy = \", 100*mean(cross_val_score(PERC, xOrignalTrain, yTrain, cv=10)))\n",
    "print(\"Perceptron - Normalized Data\", \"Accuracy = \", 100*mean(cross_val_score(PERC, xNormalizedTrain, yTrain, cv=10)))\n",
    "print(\"Perceptron - Catagorical Data\", \"Accuracy = \", 100*mean(cross_val_score(PERC, xCatagoricalTrain, yTrain, cv=10)))\n",
    "\n",
    "# SVM - Linear\n",
    "SVC = LinearSVC(random_state=0)\n",
    "print(\"SVM Linear - Orignal Data\", \"Accuracy = \", 100*mean(cross_val_score(SVC, xOrignalTrain, yTrain, cv=10)))\n",
    "print(\"SVM Linear - Normalized Data\", \"Accuracy = \", 100*mean(cross_val_score(SVC, xNormalizedTrain, yTrain, cv=10)))\n",
    "print(\"SVM Linear - Catagorical Data\", \"Accuracy = \", 100*mean(cross_val_score(SVC, xCatagoricalTrain, yTrain, cv=10)))\n",
    "\n",
    "\n",
    "print(\"Non-Linear Seperator\")\n",
    "print('--------------------')\n",
    "\n",
    "# Decision Tree. Max depth has been optimized by trying multiple depths.\n",
    "DT = tree.DecisionTreeClassifier(max_depth=10)\n",
    "print(\"Decision Tree - Orignal Data\", \"Accuracy = \", 100*mean(cross_val_score(DT, xOrignalTrain, yTrain, cv=10)))\n",
    "print(\"Decision Tree - Normalized Data\", \"Accuracy = \", 100*mean(cross_val_score(DT, xNormalizedTrain, yTrain, cv=10)))\n",
    "print(\"Decision Tree - Catagorigal Data\", \"Accuracy = \", 100*mean(cross_val_score(DT, xCatagoricalTrain, yTrain, cv=10)))\n",
    "\n",
    "# KNN. Differnt Neighbours were tried to optimized n\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "print(\"KNN - Orignal Data\", \"Accuracy = \", 100*mean(cross_val_score(KNN, xOrignalTrain, yTrain, cv=10)))\n",
    "print(\"KNN - Normalized Data\", \"Accuracy = \", 100*mean(cross_val_score(KNN, xNormalizedTrain, yTrain, cv=10)))\n",
    "print(\"KNN - Catagorical Data\", \"Accuracy = \", 100*mean(cross_val_score(KNN, xCatagoricalTrain, yTrain, cv=10)))\n",
    "\n",
    "\n",
    "# SVM RBF\n",
    "SVM = svm.SVC(random_state=0)\n",
    "print(\"SVM RBF - Orignal Data\", \"Accuracy = \", 100*mean(cross_val_score(SVM, xOrignalTrain, yTrain, cv=10)))\n",
    "print(\"SVM RBF - Normalized Data\", \"Accuracy = \", 100*mean(cross_val_score(SVM, xNormalizedTrain, yTrain, cv=10)))\n",
    "print(\"SVM RBF - Catagorical Data\",\"Accuracy = \", 100*mean(cross_val_score(SVM, xCatagoricalTrain, yTrain, cv=10)))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Probabistic Model\")\n",
    "print('-----------------')\n",
    "\n",
    "# Naive Bayes\n",
    "NB = MultinomialNB()\n",
    "print(\"NB - Original Data\", \"Accuracy = \", 100*mean(cross_val_score(NB, xOrignalTrain, yTrain, cv=10)))\n",
    "print(\"NB - Catagorical Data\", \"Accuracy = \", 100*mean(cross_val_score(NB, xCatagoricalTrain, yTrain, cv=10)))\n",
    "\n",
    "print(\"Ensemble Method\")\n",
    "print('---------------')\n",
    "\n",
    "# Random Forests. n_estimators maximied by trying different values and max depth is same thats used for DT\n",
    "RF = RandomForestClassifier(n_estimators=30, max_depth=10, random_state=0)\n",
    "print(\"Random Forests- Orignal Data\", \"Accuracy = \", 100*mean(cross_val_score(RF, xOrignalTrain, yTrain, cv=10)))\n",
    "print(\"Random Forests- Normalized Data\", \"Accuracy = \", 100*mean(cross_val_score(RF, xNormalizedTrain, yTrain, cv=10)))\n",
    "print(\"Random Forests- Catagorical Data\", \"Accuracy = \", 100*mean(cross_val_score(RF, xCatagoricalTrain, yTrain, cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "1) Catagorical Data does the best with most classifer\n",
    "\n",
    "2) SVM RBF(My proposed technique), Random Forests and Naive Bayes gives the best results using validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Trained\n"
     ]
    }
   ],
   "source": [
    "# Defining some models to compare with\n",
    "\n",
    "# NB\n",
    "NB = MultinomialNB()\n",
    "NB.fit(xCatagoricalTrain, yTrain)\n",
    "\n",
    "#KNN\n",
    "KNN = KNeighborsClassifier(n_neighbors = 3)\n",
    "KNN.fit(xCatagoricalTrain, yTrain)\n",
    "\n",
    "# RF\n",
    "RF = RandomForestClassifier(n_estimators=30, max_depth=10, random_state=0)\n",
    "RF.fit(xCatagoricalTrain, yTrain)\n",
    "\n",
    "#SVM\n",
    "SVM = svm.SVC(random_state=0, gamma = 'auto')\n",
    "SVM.fit(xCatagoricalTrain, yTrain)\n",
    "\n",
    "print(\"Models Trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracies\n",
      "\t\tRandom: Accuracy: 54.2288557214 Recall: 49.2307692308 Precision: 59.2592592593\n",
      "\t\tMode: Accuracy: 64.6766169154 Recall: 100.0 Precision: 64.6766169154\n",
      "\t\tKNN: Accuracy: 62.1890547264 Recall: 60.7692307692 Precision: 75.9615384615\n",
      "Now some good classifers\n",
      "\t\tNaive bayes: Accuracy: 72.1393034826 Recall: 80.7692307692 Precision: 77.2058823529\n",
      "\t\tRandom forest: Accuracy: 70.1492537313 Recall: 94.6153846154 Precision: 69.8863636364\n",
      "Now by best classifer\n",
      "\t\tSVM RBF: Accuracy: 73.631840796 Recall: 96.1538461538 Precision: 72.2543352601\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline accuracies\")\n",
    "print(\"\t\tRandom:\", \"Accuracy:\",(100*accuracy_score((random.randint(2, size=len(yTest))) , yTest)), \"Recall:\", 100*recall_score(yTest, (random.randint(2, size=len(yTest)))), \"Precision:\", 100*precision_score(yTest, (random.randint(2, size=len(yTest)))))\n",
    "print(\"\t\tMode:\", \"Accuracy:\",(100*accuracy_score(ones(len(yTest)) , yTest)), \"Recall:\", 100*recall_score(yTest, ones(len(yTest))), \"Precision:\", 100*precision_score(yTest, ones(len(yTest))))\n",
    "print(\"\t\tKNN:\", \"Accuracy:\", (100*KNN.score(xCatagoricalTest, yTest)), \"Recall:\",  100*recall_score(yTest,KNN.predict(xCatagoricalTest)), \"Precision:\", 100*precision_score(yTest,KNN.predict(xCatagoricalTest)))\n",
    "print(\"Now some good classifers\")\n",
    "print(\"\t\tNaive bayes:\", \"Accuracy:\", (100*NB.score(xCatagoricalTest, yTest)), \"Recall:\", 100*recall_score(yTest,NB.predict(xCatagoricalTest)), \"Precision:\", 100*precision_score(yTest,NB.predict(xCatagoricalTest)))\n",
    "print(\"\t\tRandom forest:\", \"Accuracy:\", (100*RF.score(xCatagoricalTest, yTest)),  \"Recall:\", 100*recall_score(yTest,RF.predict(xCatagoricalTest)), \"Precision:\", 100*precision_score(yTest,RF.predict(xCatagoricalTest)))\n",
    "\n",
    "\n",
    "print(\"Now by best classifer\")\n",
    "print(\"\t\tSVM RBF:\", \"Accuracy:\", (100*SVM.score(xCatagoricalTest, yTest)),  \"Recall:\", 100*recall_score(yTest,SVM.predict(xCatagoricalTest)), \"Precision:\", 100*precision_score(yTest,SVM.predict(xCatagoricalTest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am also using recall and precision as an evaluation critera given the nature of task.\n",
    "High recall would mean correctly classifying given a person was empathic. Precision would mean how many were empathic from our classification of empathic. Assuming that this model is used for classifying empathic person to be recruited. We dont want to misclassifying an empathic person, However even if we misclassify a non-empathic person, the person could be later screened in other stages of recuitment. Hence our classification critera is high recall and high accuracy overall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
